{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of BoneAmputee's CLIP-featurevis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L2Ym3X_2aJM-",
        "RCCtq-qraNjW",
        "JhVfjwjideXW",
        "G1eZO0dvevJF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walkerstipe/Surreal_GAN/blob/main/Copy_of_BoneAmputee's_CLIP_featurevis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2Ym3X_2aJM-"
      },
      "source": [
        "# [BoneAmputee](https://instagram.com/BoneAmputee)'s [CLIP-featurevis](https://github.com/openai/CLIP-featurevis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oT-tEmycZb5"
      },
      "source": [
        "### setup\n",
        "*   uses Tensorflow 1.x and Lucid 0.3.8\n",
        "*   grabs `bpe_simple_vocab_16e6.txt`, `image32.pb` and `text32.pb`\n",
        "*   includes modified `tokenizer.py` and `model.py`\n",
        "\n",
        "### change to `tokenizer.py`\n",
        "*   Azure credential issue avoided by loading bpe from local storage\n",
        "\n",
        "### change to `model.py`\n",
        "*   `graph_def` loading from protobufs in local storage\n",
        "\n",
        "### change to `example_usage.py`\n",
        "*   cat+dog image url changed to one that doesn't 404\n",
        "\n",
        "### changes to `example_facets.py`\n",
        "*   removed `()` from three instances of `@wrap_objective()`\n",
        "*   Azure credential issue avoided using `requests` to get .npy files\n",
        "*   changed `model.name` to `model.model_name`\n",
        "*   changed `/root/` to `/content/` for Colab, but disabled saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCCtq-qraNjW"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9L1mQZRZ7b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b4ece9-2243-4ee8-9c14-7d9ecff70fb6"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install --quiet ftfy regex tqdm blobfile lucid==0.3.8\n",
        "!wget -nc https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz\n",
        "![ -f \"bpe_simple_vocab_16e6.txt\" ] && echo \"skipping bpe_simple_vocab_16e6.txt\" || gunzip \"bpe_simple_vocab_16e6.txt.gz\"\n",
        "!wget -nc https://openaipublic.blob.core.windows.net/clip/tf/RN50_4x/084ee9c176da32014b0ebe42cd7ca66e/image32.pb\n",
        "!wget -nc https://openaipublic.blob.core.windows.net/clip/tf/RN50_4x/da21bc82c7bba068aa8163333438354c/text32.pb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 11.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9MB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 35.0MB/s \n",
            "\u001b[?25h  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lucid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "--2021-07-04 09:16:18--  https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/openai/CLIP/main/clip/bpe_simple_vocab_16e6.txt.gz [following]\n",
            "--2021-07-04 09:16:18--  https://raw.githubusercontent.com/openai/CLIP/main/clip/bpe_simple_vocab_16e6.txt.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1356917 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘bpe_simple_vocab_16e6.txt.gz’\n",
            "\n",
            "bpe_simple_vocab_16 100%[===================>]   1.29M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-07-04 09:16:19 (33.0 MB/s) - ‘bpe_simple_vocab_16e6.txt.gz’ saved [1356917/1356917]\n",
            "\n",
            "--2021-07-04 09:16:19--  https://openaipublic.blob.core.windows.net/clip/tf/RN50_4x/084ee9c176da32014b0ebe42cd7ca66e/image32.pb\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.150.77.132\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 349228317 (333M) [application/octet-stream]\n",
            "Saving to: ‘image32.pb’\n",
            "\n",
            "image32.pb          100%[===================>] 333.05M  35.1MB/s    in 10s     \n",
            "\n",
            "2021-07-04 09:16:29 (33.0 MB/s) - ‘image32.pb’ saved [349228317/349228317]\n",
            "\n",
            "--2021-07-04 09:16:29--  https://openaipublic.blob.core.windows.net/clip/tf/RN50_4x/da21bc82c7bba068aa8163333438354c/text32.pb\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.150.77.132\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.150.77.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 365080340 (348M) [application/octet-stream]\n",
            "Saving to: ‘text32.pb’\n",
            "\n",
            "text32.pb           100%[===================>] 348.17M  18.4MB/s    in 19s     \n",
            "\n",
            "2021-07-04 09:16:49 (18.0 MB/s) - ‘text32.pb’ saved [365080340/365080340]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUFA3CZ6aTQF"
      },
      "source": [
        "# By Alec Radford\n",
        "\n",
        "import html\n",
        "import ftfy\n",
        "import json\n",
        "import regex as re\n",
        "from functools import lru_cache\n",
        "import tensorflow as tf\n",
        "import blobfile\n",
        "\n",
        "def pad(x, pad_length = 76):\n",
        "    z = np.zeros((pad_length))\n",
        "    z[0:len(x)] = x\n",
        "    return z\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "\n",
        "    def __init__(self, bpe_path = None):\n",
        "        if bpe_path == None:\n",
        "            bpe_path = blobfile.BlobFile('bpe_simple_vocab_16e6.txt', 'r')\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
        "        merges = bpe_path.read().split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>':'<|startoftext|>', '<|endoftext|>':'<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text, n_text = 76, pad = True):\n",
        "        sot = self.encoder['<|startoftext|>']\n",
        "        eot = self.encoder['<|endoftext|>']\n",
        "        tokens = self.encode(text)\n",
        "        tokens = [sot]+tokens[:n_text-1]+[eot]\n",
        "        if pad:\n",
        "            return [tokens + [0]*(n_text+1-len(tokens))]\n",
        "        else:\n",
        "            return tokens\n",
        "\n",
        "    def sot(self):\n",
        "        return self.encoder['<|startoftext|>']\n",
        "\n",
        "    def eot(self):\n",
        "        return self.encoder['<|endoftext|>']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxsUALHeaac_"
      },
      "source": [
        "from lucid.modelzoo.vision_base import Model\n",
        "from lucid.optvis import render\n",
        "import tensorflow as tf\n",
        "from lucid.misc.io import load, save\n",
        "\n",
        "\n",
        "class CLIPImage(Model):\n",
        "    image_value_range = (0, 255)\n",
        "    input_name = 'input_image'\n",
        "    def __init__(self):\n",
        "        with tf.gfile.GFile('image32.pb', \"rb\") as f:\n",
        "          self.graph_def = tf.GraphDef()\n",
        "          self.graph_def.ParseFromString(f.read())\n",
        "        self.model_name = \"RN50_4x\"\n",
        "        self.image_shape = [288, 288, 3]\n",
        "        self.model_path = \"https://openaipublic.blob.core.windows.net/clip/tf/RN50_4x/084ee9c176da32014b0ebe42cd7ca66e/image32.pb\"\n",
        "\n",
        "    def load(self, inp = None):\n",
        "        import tensorflow as tf\n",
        "        if inp == None:\n",
        "            self.inp = tf.placeholder(shape = (None,self.image_shape[0], self.image_shape[1], 3), dtype = tf.float32)   \n",
        "        else:\n",
        "            self.inp = inp\n",
        "        self.T   = render.import_model(self, self.inp, self.inp)\n",
        "        return self.inp, self.T\n",
        "\n",
        "\n",
        "class CLIPText(Model):\n",
        "    input_name = 'tokens'\n",
        "\n",
        "    def __init__(self):\n",
        "        with tf.gfile.GFile('text32.pb', \"rb\") as f:\n",
        "          self.graph_def = tf.GraphDef()\n",
        "          self.graph_def.ParseFromString(f.read())\n",
        "        self.model_name = f\"RN50_4x_text\"\n",
        "        self.model_path = \"https://openaipublic.blob.core.windows.net/clip/tf/RN50_4x/da21bc82c7bba068aa8163333438354c/text32.pb\"\n",
        "\n",
        "    def load(self, O = None):\n",
        "        import tensorflow as tf\n",
        "        if O == None:\n",
        "            self.O = tf.placeholder(tf.int32, [None, None])  \n",
        "        else:\n",
        "            self.O = O\n",
        "        tf.import_graph_def(self.graph_def, {self.input_name: self.O}, name = \"text\")\n",
        "        gph = tf.get_default_graph()\n",
        "        self.T = lambda x: gph.get_tensor_by_name(\"text/\" + x + \":0\")\n",
        "        return self.O, self.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhVfjwjideXW"
      },
      "source": [
        "# example_usage.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXF6kNkYdi0U"
      },
      "source": [
        "# from tokenizer import SimpleTokenizer\n",
        "# from model import CLIPImage, CLIPText\n",
        "import tensorflow as tf\n",
        "from lucid.misc.io import load\n",
        "import numpy as np\n",
        "\n",
        "def imresize(img, size, scale=255):\n",
        "    from PIL import Image\n",
        "    im = Image.fromarray((img*scale).astype(np.uint8) )\n",
        "    return np.array(im.resize(size, Image.BICUBIC)).astype(np.float32)/scale\n",
        "\n",
        "tokenizer = SimpleTokenizer()\n",
        "\n",
        "tf.reset_default_graph()\n",
        "inp_text, T_text = CLIPText().load()\n",
        "inp_img,  T_img  = CLIPImage().load()\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "captions = [\"This is a dog\", \"This is a cat\", \"This is a dog and a cat\"]\n",
        "tokens   = []\n",
        "for caption in captions:\n",
        "\ttokens.append(tokenizer.tokenize(caption)[0])\n",
        "\n",
        "img    = imresize(load(\"https://i.imgur.com/EwD46UF.jpg\"), [288,288])\n",
        "\n",
        "text_embd = sess.run(T_text(\"text_post/l2_normalize\"), {inp_text: tokens})\n",
        "img_embd  = sess.run(T_img(\"l2_normalize\"), {inp_img: [img]})\n",
        "\n",
        "scores = (text_embd @ img_embd.T)[:,0]\n",
        "\n",
        "for score, caption in zip(scores, captions):\n",
        "\tprint(caption, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1eZO0dvevJF"
      },
      "source": [
        "# example_facets.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13TYSDsnexAV"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# from model import CLIPImage, CLIPText\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from lucid.optvis import objectives, param\n",
        "import lucid.optvis.render as render\n",
        "from lucid.optvis.objectives import wrap_objective, diversity\n",
        "import lucid.optvis.transform as transform\n",
        "from lucid.misc.io import load, save\n",
        "\n",
        "\n",
        "@wrap_objective\n",
        "def l2(batch=None):\n",
        "  def inner(T):\n",
        "    return -tf.reduce_mean((T(\"input\") - 0.5)**2)\n",
        "  return inner\n",
        "\n",
        "@wrap_objective\n",
        "def vector(layer, d, batch=None):\n",
        "  def inner(T):\n",
        "    channel_obj = tf.reduce_mean( tf.einsum( \"ijkl,j->ikl\", tf.nn.relu(T(layer)), tf.constant(d) ), [1,2])\n",
        "    channel_obj_weighted = tf.reduce_mean(channel_obj)**(1/1)\n",
        "    return channel_obj_weighted\n",
        "  return inner\n",
        "\n",
        "@wrap_objective\n",
        "def attr(obj, style_attrs, layers, strength):\n",
        "    def inner(T):\n",
        "        style = tf.constant(style_attrs)\n",
        "        obj_t = obj(T)  \n",
        "        layer_t = T(layers[0])\n",
        "        w = tf.linspace(strength[0], strength[1], tf.shape(layer_t)[0])\n",
        "        batch_n, _, _, _ = layer_t.get_shape().as_list()\n",
        "        style = tf.transpose(style, (0,2,3,1))\n",
        "        style = tf.image.resize(style, (tf.shape(layer_t)[2],tf.shape(layer_t)[3]))\n",
        "        style = tf.transpose(style, (0,3,1,2))\n",
        "        flat_attrs = []\n",
        "        grads = tf.gradients(obj_t, [T(layer) for layer in layers])\n",
        "        for layer, grad_t in zip(layers, grads):\n",
        "            layer_t = T(layer)\n",
        "            attr_t = layer_t * tf.nn.relu(tf.stop_gradient(grad_t))\n",
        "            if len(style_attrs.shape) == 2:\n",
        "                flat_attr_t = tf.reduce_sum(attr_t, axis=(2,3))\n",
        "            elif len(style_attrs.shape) == 4:\n",
        "                flat_attr_t = attr_t\n",
        "            flat_attrs.append(flat_attr_t)\n",
        "        flat_attr_t = tf.concat(flat_attrs, -1)\n",
        "        return tf.reduce_sum(w[:,None,None,None]*flat_attr_t*style)\n",
        "    return inner\n",
        "\n",
        "def render_facet(model, neuron_obj, layers, style_attrs, strength = (0.1, 0.3), l2_weight = 10.0, resolution = 128, alpha = False):\n",
        "\n",
        "    def mean_alpha():\n",
        "        def inner(T):\n",
        "            input_t = T(\"input\")\n",
        "            return tf.sqrt(tf.reduce_mean(input_t[..., 3:] ** 2))\n",
        "        return objectives.Objective(inner)\n",
        "\n",
        "    standard_transforms = [\n",
        "        transform.pad(2, mode='constant', constant_value=.5),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.jitter(4),\n",
        "        transform.random_scale([0.995**n for n in range(-5,80)] + [0.998**n for n in 2*list(range(20,40))]),\n",
        "        transform.random_rotate(list(range(-20,20))+list(range(-10,10))+list(range(-5,5))+5*[0]),\n",
        "        transform.jitter(2),\n",
        "        transform.crop_or_pad_to(resolution, resolution)\n",
        "    ]\n",
        "\n",
        "    if alpha:\n",
        "        standard_transforms.append(transform.collapse_alpha_random())\n",
        "        param_f = lambda: param.image(resolution, batch=9, alpha=True)\n",
        "    else:\n",
        "        param_f = lambda: param.image(resolution, batch=9)\n",
        "\n",
        "    optimizer      = tf.train.AdamOptimizer(0.02)\n",
        "    ultimate_layer = [n.name for n in model.graph_def.node if \"image_block_4\" in n.name][-1]\n",
        "    obj            = vector(ultimate_layer, neuron_obj) \n",
        "    facetsp        = [(5/len(layers))*attr(obj, style, [layer], strength) for style, layer in list(zip(style_attrs, layers))]\n",
        "    for facetp in facetsp:\n",
        "        obj = obj + facetp\n",
        "    obj = obj + l2_weight*l2()\n",
        "    if alpha:\n",
        "        obj -= mean_alpha()\n",
        "        obj -=  1e2 * objectives.blur_alpha_each_step()\n",
        "    data = render.render_vis(model, obj, param_f, transforms=standard_transforms, optimizer=optimizer, thresholds=(1024*4,))\n",
        "    return data\n",
        "\n",
        "def one_hot(ind):\n",
        "    z = np.zeros(2560)\n",
        "    z[ind] = 1\n",
        "    return z.astype(np.float32)\n",
        "\n",
        "facets = [\"face\", \"text\", \"logo\", \"pose\", \"arch\", \"nature\", \"indoor\"]\n",
        "model  = CLIPImage()\n",
        "d      = one_hot(100)\n",
        "\n",
        "for facet in facets:\n",
        "    layernames  = [n.name for n in model.graph_def.node if (\"image_block_3\" in n.name) and (\"Relu_2\" in n.name)][::2]\n",
        "    def loadnpy(url):\n",
        "        import blobfile\n",
        "        from io import BytesIO\n",
        "        # fp = blobfile.BlobFile(url, \"rb\")\n",
        "        import requests\n",
        "        x  = np.load(BytesIO(requests.get(url).content))\n",
        "        # fp.close()\n",
        "        return x\n",
        "\n",
        "    style_attrs = [loadnpy(f\"https://openaipublic.blob.core.windows.net/clip/facets/{model.model_name}/{layername}/{facet}_spatial.npy\") for layername in layernames]\n",
        "    for l2_weight in [10]:\n",
        "        img = render_facet(model, \n",
        "                           d, \n",
        "                           layernames, \n",
        "                           style_attrs, \n",
        "                           l2_weight = l2_weight, \n",
        "                           strength = (0.1, 5.0), \n",
        "                           alpha = False, \n",
        "                           resolution = 256)\n",
        "    break\n",
        "        # save(img[0][-1], f\"/content/{facet}.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpLXKBOYwkNf"
      },
      "source": [
        "#should be the actual model (which one specifically i still dont know) \n",
        "model\n",
        "\n",
        "#this is the main contribution to the Surreal GAN project\n",
        "# from keras.layers.merge import concatenate\n",
        "# stacked_model = define_stacked_model(members)\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}