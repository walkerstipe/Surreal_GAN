{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyuI7FVz7dA5",
    "outputId": "f3707437-0890-4340-cdb1-f85a15b756bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gym'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/gym.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "eiNiEpqNtO_-",
    "outputId": "f849adf9-c5b2-4405-f6f3-df5f25ec60dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anakin: \"Its working!\"\n",
      "observation space:  64\n",
      "action space:  4\n",
      "Score over time Q: 0.32535\n",
      "Score over time Q2: 0.169\n",
      "Score over time Q3: 0.0175\n",
      "Score over time Q4: 0.3645\n"
     ]
    }
   ],
   "source": [
    "#what about this as the code groundwork/simplest possible experiment/example\n",
    "#to show how we gonna do this empathy shit??? (cuz i need papers eh?)\n",
    "\n",
    "#convert to functions aka pretty-fi(AFTER VERSION CONTROLLING)\n",
    "#make sure shit works, (prob need to make the other environment too. easier in pycharm?)\n",
    "\n",
    "print('Anakin: \"Its working!\"')\n",
    "\n",
    "my_map = [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"GFFHFFFF\"\n",
    "    ]\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "#import WeightedSum \n",
    "#from gym.envs.registration import register\n",
    "#env = gym.make('FrozenLake-v0', desc='8x8')\n",
    "#env = gym.make('FrozenLake-v0', desc=my_map)\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('FrozenLake-v0', desc=custom_map)\n",
    "#env = gym.make('FrozenLake8x8-v0')\n",
    "#env = gym.make('8x8_v2')\n",
    "\n",
    "print(\"observation space: \", env.observation_space.n)\n",
    "print(\"action space: \", env.action_space.n)\n",
    "\n",
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 20000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "\n",
    "#print( \"Score over time: \" +  str(sum(rList)/num_episodes))\n",
    "\n",
    "\n",
    "#Get score\n",
    "#for playThroughs in range(10):\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        #Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "    \n",
    "print( \"Score over time Q: \" +  str(sum(rList)/num_episodes))\n",
    "    #print( Q)\n",
    "\n",
    "################################################\n",
    "#Q2 is second agent to be combined with the first\n",
    "Q2 = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q2[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q2-Table with new knowledge\n",
    "        Q2[s,a] = Q2[s,a] + lr*(r + y*np.max(Q2[s1,:]) - Q2[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "    \n",
    "#Get score\n",
    "#for playThroughs in range(10):\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q2[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        #Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "print( \"Score over time Q2: \" +  str(sum(rList)/num_episodes))\n",
    "##############################################################\n",
    "\n",
    "################################################\n",
    "#Now we just try and run it\n",
    "Q3 = (Q * Q2) / 2\n",
    "#for playThroughs in range(10):\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q3[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        #Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "print( \"Score over time Q3: \" +  str(sum(rList)/num_episodes))\n",
    "\n",
    "# =============================================================================\n",
    "# print(\"Q[0][0]\", Q[0][0] )\n",
    "# print((Q[0][0] + Q2[0][0])/2)\n",
    "# print((Q[0][0]))\n",
    "# =============================================================================\n",
    "\n",
    "#Now we just try and run it\n",
    "def WeightedSum(Q, Q2):\n",
    "  Q_ = np.copy(Q)\n",
    "  for y in range(len(Q[:,:])):\n",
    "    for x in range(len(Q[0])):\n",
    "      #print(x, ' ', y)\n",
    "      Q_[y][x] = (Q[y][x] + Q2[y][x]) / 2\n",
    "  return Q_\n",
    "Q4 =  WeightedSum(Q, Q2)\n",
    "#Q4 = WeightedSum.wsum(Q, Q2)\n",
    "#print(Q4)\n",
    "#for playThroughs in range(10):\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q4[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        #Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "print( \"Score over time Q4: \" +  str(sum(rList)/num_episodes))\n",
    "# =============================================================================\n",
    "# print(Q)\n",
    "# print(Q2)\n",
    "# print(Q3)\n",
    "# print(Q4)\n",
    "# =============================================================================\n",
    "# ===================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so... everythings working... but scores are wildly different depending on \n",
    "#environment...?\n",
    "\n",
    "#increase number of episodes?\n",
    "#score is just a feature of the environment, (optimality can just be low right?...)\n",
    "#this implementation is shitty, (Seems fine, but let's make a NN anyhow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFHF\n",
      "HFHFF\n",
      "HFFFH\n",
      "HHHFH\n",
      "HFFFG\n"
     ]
    }
   ],
   "source": [
    "custom_map = [\n",
    "    'SFFHF',\n",
    "    'HFHFF',\n",
    "    'HFFFH',\n",
    "    'HHHFH',\n",
    "    'HFFFG'\n",
    "]\n",
    " \n",
    "env = gym.make('FrozenLake-v0', desc=custom_map)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tts7VhTx_xD5",
    "outputId": "a0d8ee35-ae31-4e60-ee77-d96e9b7daca0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.904761904761905"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1300000/21000\n",
    "#for every 1 ai nerd in the U.S. ... there are 62 lawyers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "DvvV3HRhub0E",
    "outputId": "51d86c8a-1289-47dc-a37f-4d08e04d7b6d"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e707139f8f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Q2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Make this warning show up first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mitems\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "np.shape(Q2)\n",
    "print(np.average(Q,Q2))\n",
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JVTvEEEuztF"
   },
   "outputs": [],
   "source": [
    "def WeightedSum(Q, Q2):\n",
    "  Q_ = np.copy(Q)\n",
    "  for y in range(len(Q[:,:])):\n",
    "    for x in range(len(Q[0])):\n",
    "      #print(x, ' ', y)\n",
    "      Q_[y][x] = (Q[y][x] + Q2[y][x]) / 2\n",
    "  return Q_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fgIi4B7v8Nt",
    "outputId": "20aeab32-176d-43bf-acde-8d6d86271d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.15736105e-03 4.86327405e-03 1.09712950e-02 4.31863486e-03]\n",
      " [1.61097671e-03 1.26546630e-02 4.50838772e-03 4.38152866e-03]\n",
      " [1.99446843e-03 5.30380375e-03 1.18162058e-02 2.04051806e-03]\n",
      " [1.93062346e-03 8.25649649e-03 3.97368788e-02 2.11669317e-03]\n",
      " [1.23214516e-03 1.65774257e-03 3.71109168e-02 2.59399717e-03]\n",
      " [1.36296676e-03 3.76915773e-02 2.77524536e-02 4.20873072e-03]\n",
      " [2.48791165e-03 4.73483853e-02 1.18903142e-01 3.56701667e-04]\n",
      " [2.00953406e-03 5.56591154e-02 2.32590150e-03 1.54895834e-03]\n",
      " [1.51318666e-03 2.51134938e-03 3.20992700e-03 1.04747198e-02]\n",
      " [1.61666464e-03 3.26641269e-03 3.87170178e-03 1.37676589e-02]\n",
      " [1.43664380e-03 2.04069496e-03 2.13749430e-03 2.06068612e-02]\n",
      " [2.58040224e-04 4.98504135e-04 1.99240513e-04 5.91646740e-02]\n",
      " [1.51713719e-03 1.36876704e-03 1.19855189e-03 4.32706531e-02]\n",
      " [2.74169402e-02 1.81542532e-03 1.31362345e-03 3.29489496e-02]\n",
      " [1.94836234e-03 2.51587751e-03 1.22792353e-01 1.22223875e-03]\n",
      " [2.02590532e-03 1.03820304e-03 5.04663901e-02 1.07746051e-03]\n",
      " [2.56934403e-03 2.12725391e-03 2.49091845e-03 3.74384642e-03]\n",
      " [3.97480287e-03 1.88020322e-03 2.67664537e-03 1.75955818e-03]\n",
      " [5.51389869e-03 3.15324456e-04 7.26931931e-05 7.42539099e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.07610993e-04 1.42930507e-04 8.51984993e-03 1.78556385e-04]\n",
      " [3.78896694e-04 1.83645830e-04 1.34136646e-04 1.09026794e-02]\n",
      " [6.81046004e-02 2.22340936e-03 2.43145791e-02 8.12521619e-04]\n",
      " [1.10213330e-03 1.48661667e-02 1.69767703e-02 2.05401055e-03]\n",
      " [1.91366618e-03 1.58232736e-03 1.60926746e-03 9.89650755e-03]\n",
      " [1.05968208e-03 7.45136375e-04 1.08436046e-03 9.20607783e-04]\n",
      " [8.08490096e-04 3.26968189e-04 1.69236087e-04 5.79201626e-04]\n",
      " [1.20422906e-04 4.47648546e-04 1.05882101e-05 2.00265120e-04]\n",
      " [9.61288174e-03 2.68633703e-05 7.75030029e-05 5.12531811e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.25651346e-04 2.19640169e-04 1.60690700e-01 3.83774069e-05]\n",
      " [1.15220790e-03 2.09382331e-01 0.00000000e+00 1.10251710e-03]\n",
      " [5.29315721e-04 7.95693997e-04 1.13541822e-03 1.68059201e-03]\n",
      " [3.59651965e-05 5.91350493e-05 5.49163582e-04 8.27836439e-04]\n",
      " [6.61827006e-04 5.41244137e-05 9.51154357e-07 4.23508672e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.81354096e-04 2.54430595e-04 1.06087884e-01 4.88964835e-05]\n",
      " [2.70752546e-05 7.91319400e-02 7.60221212e-05 3.83617368e-04]\n",
      " [1.31076062e-05 5.34070592e-04 6.36416071e-04 1.47937819e-01]\n",
      " [0.00000000e+00 2.23819383e-01 1.38973073e-03 1.24500251e-03]\n",
      " [4.23710504e-04 4.01323498e-04 2.97726563e-04 4.26530838e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.37062190e-06 2.94156550e-06 8.72345708e-05 1.81030233e-12]\n",
      " [1.53768602e-05 1.10675446e-04 4.17568674e-05 2.41669676e-02]\n",
      " [7.86764196e-02 1.92117686e-04 3.47640628e-06 1.60017319e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 8.96532480e-05 5.01985342e-01 0.00000000e+00]\n",
      " [4.11192097e-04 3.33963291e-04 4.18379764e-04 6.60847199e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.10139762e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [8.45340432e-02 0.00000000e+00 8.59628738e-05 3.53300811e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 8.41037059e-01 0.00000000e+00]\n",
      " [4.02599384e-04 2.68560206e-04 4.09003177e-04 3.88440171e-04]\n",
      " [0.00000000e+00 2.01805733e-04 1.47307106e-04 2.29087938e-06]\n",
      " [1.07424453e-04 1.41347964e-04 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 7.48559380e-05 5.45521311e-04 0.00000000e+00]\n",
      " [1.77783867e-04 4.67925776e-01 4.67723540e-04 1.18896576e-04]\n",
      " [0.00000000e+00 1.30369052e-04 9.98411778e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(WeightedSum(Q, Q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCkdgCfvu_eF",
    "outputId": "91ef9992-e876-45d6-fd7c-3149376ea4c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006507379454077243"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[0][1]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Simple_Agent_Fusion_QLearning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
